{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AhbnkHs0Vn65"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    " \n",
    " \n",
    "env = gym.make('LunarLander-v2')\n",
    " \n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    " \n",
    "plt.ion()\n",
    " \n",
    "# if gpu is to be used\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rj5e5QSMVn7D"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.longtermpos = round(self.capacity * 0.05)\n",
    "        self.longtermfull = False\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        # print(self.position)\n",
    "        # print('aa')\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "          \n",
    "        if self.longtermfull:\n",
    "            if self.position < self.longtermpos:\n",
    "                self.position = self.longtermpos\n",
    "        else:\n",
    "            if len(self.memory) > self.longtermpos:\n",
    "                self.longtermfull = True\n",
    "                print(\"Long term full\")\n",
    "        \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % (self.capacity)\n",
    "        \n",
    "                \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S-Z3Syq7Vn7J"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, fc1_dims, fc2_dims, \n",
    "            n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dims = [input_dims]\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        \n",
    "        # The 3 layers in input,output fashion\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
    "        self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
    "        self.bn3 = nn.BatchNorm1d(self.n_actions)\n",
    "        self.head = nn.Linear(self.n_actions, self.n_actions)\n",
    "        # Mean squared error\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "#         # uses gpu if it can\n",
    "#         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.device = \n",
    "#         self.to(self.device)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def __call__(self, x):\n",
    "        \n",
    "#         converts\n",
    "        self.eval()\n",
    "        try:\n",
    "            x = torch.from_numpy(x)\n",
    "        except:\n",
    "            pass\n",
    "#         print(x)\n",
    "#         print(';;;;;')\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.head(x.view(x.size(0), -1))\n",
    "        \n",
    "\n",
    "\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         actions = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkzoIUNPVn7P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rnnt3vwXVn7U"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "EPS_START = 1\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2000\n",
    "TARGET_UPDATE = 300\n",
    " \n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "input_dims = 8\n",
    " \n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    " \n",
    "policy_net = DQN(n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=256, fc2_dims=256)\n",
    "target_net = DQN(n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=256, fc2_dims=256)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    " \n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    " \n",
    "memory = ReplayMemory(100_000)\n",
    " \n",
    " \n",
    " \n",
    "steps_done = 0\n",
    " \n",
    " \n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#     print(eps_threshold)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    " \n",
    "#             a = policy_net(state)\n",
    "#             print(a)\n",
    "#             a = a.max(1)\n",
    "#             print(a)\n",
    "#             a = a[1]\n",
    "#             print(a)\n",
    "#             a = a.view(1,1)\n",
    "#             print(a)\n",
    "            \n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    " \n",
    " \n",
    "episode_scores = []\n",
    " \n",
    " \n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(scores_t) >= 100:\n",
    "        means = scores_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    " \n",
    "    plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "#         display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    filename = \"tailorgraph.png\"\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mwK8Z7TyVn7Z"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "#     print(batch.state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "#     state_batch = batch.state\n",
    "    action_batch = torch.cat(batch.action)\n",
    "#     action_batch = batch.reward\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "#     reward_batch = batch.reward\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "#     print(state_batch)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "#         print(param)\n",
    "        \n",
    "        try:\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        except:\n",
    "            pass\n",
    "#             print(\"above doesn't work\")\n",
    "        \n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FUpe2-ybVn7d",
    "outputId": "4cfd463e-8ace-4bdd-f072-a6c2731c0a3b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATjklEQVR4nO3df7DldX3f8efLXeWHJLDIUlZRF6U2BodZ5VbHCTEkIkFala3O0NYgjZOCTahtWqsYYrIEO1O34LaJqQFaOjQYxITRKiRRliT+yIh6V5ZlMUUB14QfTe/6GxBk8d0/znfD4X7O3b279577vbvn+Zj5zv1+P5/P95z3Z3fmvu73x/meVBWSJA17Wt8FSJKWH8NBktQwHCRJDcNBktQwHCRJDcNBktQwHKT9kGRFkoeSPG8xx0rLRfycgyZBkoeGNg8HHgOe6LYvqKoPLX1V0vJlOGjiJNkB/FJVbd7DmJVVtWvpqpKWF08rSUCS9ya5Psl1Sb4P/EKSVya5Ncl3kjyY5LeTPL0bvzJJJVnbbV/b9f9Jku8n+XySE/Z1bNf/2iRfTfLdJL+T5C+T/Iul/RfRpDMcpCetB/4AOBK4HtgF/BvgGOCngDOBC/aw/z8H3gMcDfw1cOm+jk1yLPAR4D907/t14OX7OyFpfxkO0pM+V1WfqKofVdUPqupLVfWFqtpVVfcCVwI/s4f9/6iqpqvqceBDwLr9GPuPga1V9b+7vk3AzoVPTdo3K/suQFpG/mZ4I8lPAJcDpzC4iL0S+MIe9v+/Q+uPAEfsx9hnD9dRVZXkvr1WLi0yjxykJ82+O+MKYDtwYlX9OPAbQMZcw4PA8bs3kgR4zpjfU2oYDtLcfgz4LvBwkhez5+sNi+VG4GVJXpdkJYNrHquX4H2lpzAcpLn9e+A84PsMjiKuH/cbVtXfAucA7we+CbwQuI3B5zJIclqS7+wen+Q9ST4xtP2pJO8cd506+Pk5B2kZS7ICeAB4U1V9tu96NDk8cpCWmSRnJjkyySEMbnfdBXyx57I0YQwHafk5FbiXwS2sZwJnV9Vj/ZakSeNpJUlSwyMHSVLjoPgQ3DHHHFNr167tuwxJOqBs2bJlZ1WNvFX6oAiHtWvXMj093XcZknRASfKNufo8rSRJahgOkqSG4SBJahgOkqSG4SBJahgOkqSG4SBJahgOkqSG4SBJahgOkqSG4SBJahgOkqRGL+GQZEOS+5Ns7ZazuvZnJfnzJA8l+UAftUmS+n0q66aqumxW26MMvhbxJd0iSerBsjqtVFUPV9XnGISEJKknfYbDhUm2Jbk6yap93TnJ+Ummk0zPzMyMoz5JmlhjC4ckm5NsH7G8Afgg8EJgHfAgcPm+vn5VXVlVU1U1tXr1yC8ykiTtp7Fdc6iq0+czLslVwI3jqkOStO/6ultpzdDmemB7H3VIkkbr626ljUnWAQXsAC7Y3ZFkB/DjwDOSnA2cUVVf6aNISZpUvYRDVZ27h761S1iKJGmEZXUrqyRpeTAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEkNw0GS1DAcJEmNXsIhyYYk9yfZ2i1nde2vSbIlyR3dz5/roz5JmnQre3zvTVV12ay2ncDrquqBJC8BPgk8Z+lLk6TJ1mc4NKrqtqHNO4FDkxxSVY/1VZMkTaI+rzlcmGRbkquTrBrR/0bgtrmCIcn5SaaTTM/MzIy3UkmaMKmq8bxwshk4bkTXxcCtDE4hFXApsKaq3jq070nAx4Ezquqevb3X1NRUTU9PL0rdkjQpkmypqqlRfWM7rVRVp89nXJKrgBuHto8HPgq8ZT7BIElafH3drbRmaHM9sL1rPwq4CXh3Vf1lH7VJkvq75rCxu111G/CzwK927RcCJwLvGbrN9dieapSkidXL3UpVde4c7e8F3rvE5UiSZvET0pKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWoYDpKkhuEgSWr0Eg5JNiS5P8nWbjmra3/5UNvtSdb3UZ8kTbqVPb73pqq6bFbbdmCqqnYlWQPcnuQTVbWrh/okaWL1GQ6NqnpkaPNQoPqqRZImWZ/XHC5Msi3J1UlW7W5M8ookdwJ3AG+b66ghyflJppNMz8zMLFXNkjQRUjWeP86TbAaOG9F1MXArsJPBkcGlwJqqeuus/V8MXAO8qqoe3dN7TU1N1fT09KLULUmTIsmWqpoa1Te200pVdfp8xiW5CrhxxP5/leRh4CWAv/klaQn1dbfSmqHN9QwuRJPkhCQru/XnA/8A2LHkBUrShOvrgvTGJOsYnFbaAVzQtZ8KXJTkceBHwC9X1c5+SpSkydVLOFTVuXO0/z7w+0tcjiRpFj8hLUlqzDsckpya5Be79dVJThhfWZKkPs0rHJL8JvAu4N1d09OBa8dVlCSpX/M9clgPvB54GKCqHgB+bFxFSZL6Nd9w+GENPi1XAEmeOb6SJEl9m284fCTJFcBRSf4lsBm4anxlSZL6NK9bWavqsiSvAb7H4INpv1FVN4+1MklSb/YaDklWAJ/sHodhIEjSBNjraaWqegJ4JMmRS1CPJGkZmO8npB8F7khyM90dSwBV9faxVCVJ6tV8w+GmbpEkTYD5XpC+JskzgBd1TXdV1ePjK0uS1Kd5hUOS0xh88c4OIMBzk5xXVZ8ZX2mSpL7M97TS5cAZVXUXQJIXAdcBp4yrMElSf+b7Ibin7w4GgKr6KoPnK0mSDkLzPXKYTvI/ePK7Ft4MbBlPSZKkvs03HP4V8CvA2xlcc/gM8N/GVZQkqV/zDYeVwH+tqvfD331q+pCxVSVJ6tV8rzncAhw2tH0Yg4fvSZIOQvMNh0Or6qHdG9364eMpSZLUt/mGw8NJXrZ7I8kU8IPxlCRJ6tt8rzn8W+APkzzA4At/ng2cM7aqJEm92uORQ5J/mOS4qvoS8BPA9cAu4E+Bry9BfZKkHuzttNIVwA+79VcCvwb8LvBt4Mox1iVJ6tHeTiutqKpvdevnAFdW1Q3ADUm2jrc0SVJf9nbksCLJ7gB5NfBnQ33zvV4hSTrA7O0X/HXAp5PsZHB30mcBkpwIfHfMtUmSerLHcKiq/5jkFmAN8Kmqqq7racC/HndxkqR+zOc7pG+tqo9W1fDXg361qr68v2+aZEOS+5Ns7ZazZvU/L8lDSd6xv+8hSdp/fV432FRVl83VB/zJUhYjSXrSsruonORs4F7g4b2NlSSNx3wfnzEOFybZluTqJKsAkjwTeBdwyd52TnJ+kukk0zMzM+OuVZImytjCIcnmJNtHLG8APgi8EFgHPMjga0hhEAqbhh/yN5equrKqpqpqavXq1eOahiRNpLGdVqqq0+czLslVwI3d5iuANyXZCBwF/CjJo1X1gTGVKUkaoZdrDknWVNWD3eZ6YDtAVf300JgNwEMGgyQtvb4uSG9Mso7BE153ABf0VIckaYRewqGqzp3HmA1LUIokaYQ+71aSJC1ThoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqWE4SJIahoMkqdFLOCTZkOT+JFu75ayufW2SHwy1/14f9UnSpFvZ43tvqqrLRrTfU1XrlrwaSdLf8bSSJKnRZzhcmGRbkquTrBpqPyHJbUk+neSn59o5yflJppNMz8zMLEG5kjQ5UlXjeeFkM3DciK6LgVuBnUABlwJrquqtSQ4BjqiqbyY5BfgYcFJVfW9P7zU1NVXT09OLOwFJOsgl2VJVU6P6xnbNoapOn8+4JFcBN3b7PAY81q1vSXIP8CLA3/yStIT6ultpzdDmemB71746yYpu/QXA3wfuXfoKJWmy9XW30sYk6xicVtoBXNC1vwr4rSS7gCeAt1XVt/opUZImVy/hUFXnztF+A3DDEpcjSZrFW1klSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLUMBwkSQ3DQZLU6CUckmxIcn+Srd1y1lDfyUk+n+TOJHckObSPGiVpkq3s8b03VdVlww1JVgLXAudW1e1JngU83kt1kjTB+gyHUc4AtlXV7QBV9c2e65GkidTnNYcLk2xLcnWSVV3bi4BK8skkX07yzrl2TnJ+kukk0zMzM0tTsSRNiLGFQ5LNSbaPWN4AfBB4IbAOeBC4vNttJXAq8Obu5/okrx71+lV1ZVVNVdXU6tWrxzUNSZpIYzutVFWnz2dckquAG7vN+4BPV9XOru+PgZcBt4ylSEnSSH3drbRmaHM9sL1b/yRwcpLDu4vTPwN8Zanrk6RJ19cF6Y1J1gEF7AAuAKiqbyd5P/Clru+Pq+qmnmqUpInVSzhU1bl76LuWwe2skqSe+AlpSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNQwHSVLDcJAkNVJVfdewYElmgG/0Xcd+OAbY2XcRS8w5T4ZJm/OBOt/nV9XI7zw4KMLhQJVkuqqm+q5jKTnnyTBpcz4Y5+tpJUlSw3CQJDUMh35d2XcBPXDOk2HS5nzQzddrDpKkhkcOkqSG4SBJahgOY5bk6CQ3J/la93PVHOPO68Z8Lcl5I/o/nmT7+CteuIXMOcnhSW5K8n+S3JnkPy1t9fOX5MwkdyW5O8lFI/oPSXJ91/+FJGuH+t7dtd+V5OeXsu6F2N85J3lNki1J7uh+/txS176/FvL/3PU/L8lDSd6xVDUviqpyGeMCbAQu6tYvAt43YszRwL3dz1Xd+qqh/n8C/AGwve/5jHvOwOHAz3ZjngF8Fnht33MaUf8K4B7gBV2dtwM/OWvMLwO/163/U+D6bv0nu/GHACd0r7Oi7zmNec4vBZ7drb8EuL/v+Yx7zkP9NwB/CLyj7/nsy+KRw/i9AbimW78GOHvEmJ8Hbq6qb1XVt4GbgTMBkhwB/DvgvUtQ62LZ7zlX1SNV9ecAVfVD4MvA8UtQ8756OXB3Vd3b1flhBvMeNvzv8EfAq5Oka/9wVT1WVV8H7u5eb7nb7zlX1W1V9UDXfidwaJJDlqTqhVnI/zNJzmbwh8+dS1TvojEcxu/vVdWDAN3PY0eMeQ7wN0Pb93VtAJcClwOPjLPIRbbQOQOQ5CjgdcAtY6pzIfZa//CYqtoFfBd41jz3XY4WMudhbwRuq6rHxlTnYtrvOSd5JvAu4JIlqHPRrey7gINBks3AcSO6Lp7vS4xoqyTrgBOr6ldnn8fs27jmPPT6K4HrgN+uqnv3vcKx22P9exkzn32Xo4XMedCZnAS8DzhjEesap4XM+RJgU1U91B1IHFAMh0VQVafP1Zfkb5OsqaoHk6wB/t+IYfcBpw1tHw/8BfBK4JQkOxj8Xx2b5C+q6jR6NsY573Yl8LWq+i+LUO443Ac8d2j7eOCBOcbc14XdkcC35rnvcrSQOZPkeOCjwFuq6p7xl7soFjLnVwBvSrIROAr4UZJHq+oD4y97EfR90eNgX4D/zFMvzm4cMeZo4OsMLsiu6taPnjVmLQfOBekFzZnB9ZUbgKf1PZc9zHElg3PJJ/DkhcqTZo35FZ56ofIj3fpJPPWC9L0cGBekFzLno7rxb+x7Hks151ljNnCAXZDuvYCDfWFwvvUW4Gvdz92/AKeA/z407q0MLkzeDfziiNc5kMJhv+fM4C+zAv4K2Notv9T3nOaY51nAVxnczXJx1/ZbwOu79UMZ3KVyN/BF4AVD+17c7XcXy/BurMWeM/DrwMND/6dbgWP7ns+4/5+HXuOACwcfnyFJani3kiSpYThIkhqGgySpYThIkhqGgySpYThIIyR5IsnWoaV5Gues8W9L8pZFeN8dSY5Z6OtIC+WtrNIISR6qqiN6eN8dwFRV7Vzq95aGeeQg7YPuL/v3Jflit5zYtW/Y/bz+JG9P8pUk25J8uGs7OsnHurZbk5zctT8ryaeS3JbkCoae05PkF7r32JrkiiQrepiyJpThII122KzTSucM9X2vql4OfAAY9eyni4CXVtXJwNu6tksYPIn0ZODXgP/Vtf8m8LmqeinwceB5AEleDJwD/FRVrQOeAN68uFOU5uaD96TRftD9Uh7luqGfm0b0bwM+lORjwMe6tlMZPKqaqvqz7ojhSOBVDL7Miaq6Kcm3u/GvBk4BvtQ90fMwRj/AUBoLw0HadzXH+m7/iMEv/dcD7+keU72nRz+Peo0A11TVuxdSqLS/PK0k7btzhn5+frgjydOA59bg2+zeyeBppEcAn6E7LZTkNGBnVX1vVvtrGTyhFgYPLHxTkmO7vqOTPH+Mc5KewiMHabTDkmwd2v7Tqtp9O+shSb7A4I+rfzZrvxXAtd0pozD4spfvJNkA/M8k2xh8q9953fhLgOuSfBn4NPDXAFX1lSS/DnyqC5zHGTwa+huLPVFpFG9llfaBt5pqUnhaSZLU8MhBktTwyEGS1DAcJEkNw0GS1DAcJEkNw0GS1Pj/Ru+0o0A5v8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -53.48 average score -53.48 memory pos 64.00\n",
      "episode  1 score -450.35 average score -251.92 memory pos 156.00\n",
      "episode  2 score -79.72 average score -194.52 memory pos 231.00\n",
      "episode  3 score -87.80 average score -167.84 memory pos 331.00\n",
      "episode  4 score -207.50 average score -175.77 memory pos 428.00\n",
      "episode  5 score -295.35 average score -195.70 memory pos 531.00\n",
      "episode  6 score -147.05 average score -188.75 memory pos 632.00\n",
      "episode  7 score -178.30 average score -187.44 memory pos 753.00\n",
      "episode  8 score -478.58 average score -219.79 memory pos 912.00\n",
      "episode  9 score -65.16 average score -204.33 memory pos 1116.00\n",
      "episode  10 score -135.37 average score -198.06 memory pos 1381.00\n",
      "episode  11 score -303.78 average score -206.87 memory pos 1470.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe48e8f3dd5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mepisode_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b2859ea8a3a7>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# Optimize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 9999999999999\n",
    "# scores = []\n",
    "score = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = torch.from_numpy(env.reset())\n",
    "    state = torch.unsqueeze(state, 0)\n",
    "    \n",
    "    score = 0\n",
    "    for t in count():\n",
    "        # env.render()\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action.item())\n",
    "        score = score + reward\n",
    "        new_state = torch.from_numpy(new_state)\n",
    "        new_state = torch.unsqueeze(new_state, 0)\n",
    "#         print(new_state)\n",
    "        reward = torch.tensor([float(reward)], device=device)\n",
    "\n",
    "        memory.push(state, action, new_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = new_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_scores.append(score)\n",
    "            if i_episode % 50 == 0:\n",
    "                plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    avg_score = np.mean(episode_scores[-100:])\n",
    "    \n",
    "    print('episode ', i_episode, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score, 'memory pos %.2f' % memory.position)\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "#         math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#     print(eps_threshold)\n",
    "#     print(scores)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plot_durations()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kmq9L_8aVn7i"
   },
   "outputs": [],
   "source": [
    "pip install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sIWtEW8Vn7k"
   },
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_op1ZcKQVn7m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tailoring official.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
